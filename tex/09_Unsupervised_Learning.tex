% !TEX root = ../notes.tex

% ================  Unsupervised Learning ==============

\section{Unsupervised Learning}

In this section we will look at how machines can learn from data without human intervention as opposed to \emph{Supervised Learning}. The goal of Unsupervised Learning is to go from our dataset $X$ to a function $f(X)$, a simpler representation of the data. We can already categorize Unsupervised Learning in two categories :
\begin{itemize}
	\item Clustering for discrete data.
	\item Matrix factorization, Kalman filtering, unsupervised neural networks, etc... for continuous data.
\end{itemize}

\subsection{Clustering}

Going from a set of points with a distance metric between points to \textbf{groups} into some clusters. The goal is to perform same labelling for members that are close/similar to each other (Closeness can take on many shapes as we will see later), therefore members belonging to different clusters should be dissimilar. 

The main use cases of clustering is having points in a high-dimensional space. Similarity/Distance is defined using some distance measures such as Euclidian, Cosine, Jaccard, edit distance... The choice of such metrics is determinant regarding the performance and accuracy of the clustering, it is consequently a huge advantage to have a domain expert/specialist in order to choose the better metric.

\subsubsection{Characteristics of Clustering Methods}

In order to better understanding clustering and when/how it should be applied, we have to take a peek at the characteristics of Clustering methods, namely :
\begin{itemize}
	\item \textbf{Quantitative} : scalability (how many samples), dimensionality (how many features).
	\item \textbf{Qualitative} : Type of attributes (numerical, categorical, etc.), type of shapes (spheres, hyperplanes, etc.).
	\item \textbf{Robustness} : sensitivity to noise and outliers, sensitivity to the processing order.
	\item \textbf{User interaction} : incorporation of user constraints(e.g., number of clusters, max size of clusters), interpretability and usability i.e. how transparent is the model/ clusterization, black/white boxing c.f. trees/supervised learning.
\end{itemize}

These characteristics influence a great deal over clustering, and even by having them well defined clustering tasks are often found hard in easy looking situations.

Example with outliers (insert picture) Clustering is an iterative process, in some cases it can get worse with every iterations, as we can see in the picture, well defined at first clusters iteratively shift towards outliers, yielding a worse result each time.

\subsubsection{Examples of Clustering}


\textbf{Stereotypical Clustering}
Insert image
In high dimensionality clustering what may seem to be a right choice in high dimensionality may get bad when projected in 2D, where outliers often appear. Usually to obtain a good clusterisation, we have to accept that some points may not be classified well or even not classified at all because ultimately, the model and it's programmer cannot understand the process that generated the data.

\textbf{Clustering for Segmentation}
Insert Image, In the case of image it's breaking it apart into regions of similar points.

\textbf{Condensation/Compression} Insert image. In this image the human can infer several models/distributions that are contained into the data (linear relations). Condensation/Compression doesn't aim to cluster/model the underlying structure, but try to identify and subtract "submodels" in order to yield a simplification, a reduced variance "sample" of the data.

\subsubsection{Cluster Bias}

What is called Cluster Bias or Cluster hallucination comes from the human way of conceptualizing and categorize the world into categories (\emph{exemplars}); we often tend to see cluster structures where there is not really (ex : constellations). Nowadays, unsupervised learning is rather cheap, thus, combined with the cluster bias, instances of overuse or unjustified use of clustering are often encountered. A good data scientist has to ask himself if he has meaningful reasons to use clustering. People often assume that domain has discrete classes in it (ex. types of people). In reality the data is usually continuous. 

\subsubsection{Terminology}
There are several way to organize/apply clustering :
\begin{itemize}
	\item \textbf{Hierarchical clustering: } Cluster are hierarchically related. Such construction can be made by using a bottom-up or a top-down approach.
	\item \textbf{Flat clustering: } As opposed to hierarchical clustering, there is no inter-cluster structure.
	\item \textbf{Hard clustering: } Every item is uniquely assigned to a cluster.
	\item \textbf{Soft clustering: } cluster membership is a real valued function, distributed across several clusters. This type of clustering is very agile and useful in many cases.
\end{itemize}

\subsubsection{Clustering : A hard problem.}
Here our eternal curse of dimensionality strikes again. The data scientist is often tricked by how clustering looks easy when there are only two dimensions or when there is a small amount of data and it is in most cases true. Nevertheless applications involve tremendously more than two dimensions and hight dimension spaces look very different from 2D spaces, for example, all points in high dimensions are about the same distance from each other. 

Examples SKIPPED IN CLASS.

\textbf{Music CDs}

In the example of the Music CDs, the representation is very good but the dimensionality explodes quickly (ex: Amazon), therefore the choosing of the encoding is a determinant factor. 

\textbf{Documents}

Encoding has a big impact on clustering, in this case the way we define a document as sets of words has a direct impact on the distance metric :
\begin{itemize}
	\item Sets as vectors : cosine distance
	\item Sets as sets : Jaccard distance
	\item Sets as points Euclidian distance
\end{itemize}

\subsubsection{Methods of Clustering}
A quick peek at the several ways to apply clustering.

\paragraph{Point assignment}

A set of clusters is defined and maintained. Then each point belongs to the "nearest" cluster.

\paragraph{Hierarchical Clustering}

Hierarchical clustering can be done in two ways :

\textbf{Agglomerative: } (bottom up approach) We start with each point being a cluster then repeatedly combine the  two "nearest" clusters into one.

\textbf{Divisive: } (top down approach) Starting with a single cluster including all the points, we recursively split it into smaller clusters.

The key operation of hierarchical clustering is how to repeatedly combine two nearest clusters. There are also several other questions that also arise when speaking about hierarchical clustering such as :
\begin{description}
	\item [How is a cluster of more than one point represented ?]
The key problem is how the "location" of each cluster represented throughout the merging process in order to choose which pair of cluster is closest. In the \textbf{Euclidean case}, each cluster has a \emph{centroid} computed from the average of its data points, note that a centroid doesn't have to be a data point it can be totally artificial. We can also use a \emph{clustroid}, namely the existing data point "closest" to all of the other points according to the following metrics :
\begin{itemize}
	\item Smallest maximum distance to other points.
	\item Smallest average distance to other points.
	\item Smallest sum of squares of distances to other points.
\end{itemize}
	\item [How is the "nearness" metric of cluster determined ?]
We can measure the nearness between clusters according to different approaches:
\begin{enumerate}
	\item \emph{Intercluster distance} : Minimum of the distances between any two points, one from each cluster.
	\item \emph{Cohesion} : Pick a metric e.g. maximum distance from the clustroid for example. And then merge clusters whose union is most cohesive. Cohesion can also be : \emph{diameter} of the merged cluster (max distance between points in the cluster), \emph{average distance} between points in the cluster, \emph{density-based approach} (ex: for geolocation data) see after.
\end{enumerate} 
	\item [When should we stop combining clusters ?]
\end{description}

Example insert image with dendogram

\textbf{Implementation of hierarchical clustering}: for naive implementation, the pairwise distances between all pairs of clusters must be computed and a merge operation must be performed, totalling to a complexity of $O(N^{3})$, more clever approaches using better indexing structures such as priority queues can improve performance up to $O(N^{2}log(n))$ complexity. Nevertheless, it is still too complex for big dataset not fitting into memory.

\paragraph{K-means Clustering}

The standard algorithm is based on \textbf{Euclidean distance}, the quality measure is \textbf{intra cluster} only. It is a simple greedy algorithm that locally optimizes the quality measure. The algorithm resumes to the following two steps :
\begin{itemize}
	\item \emph{Finding the closest cluster center} for each item and assign it to that cluster.
	\item \emph{Recompute the cluster centroid} as the mean of the items, having added the new item to the cluster.
\end{itemize}
The most challenging and impactful decisions when using K-means is to choose the number of clusters and also to pick the initial cluster centres. The latter can be done by sampling the input data or following more complex methods as described later. We also have to choose when to stop iterating by either :
\begin{itemize}
	\item Defining a fixed number of iterations.
	\item Until no changes in assignments happen.
	\item Until there are only small changes in quality.
\end{itemize}

As we said previously, \textbf{Initialization} is the most crucial part when running K-mean, we can choose to pick a random subset of k points from dataset or use methods like K-Means++ that iteratively constructs a random sample with good spacing across the dataset. However you choose the method for choosing your initial points, take into account the fact that optimal K-Means clustering is a NP-hard problem and that randomization helps avoiding bad configurations and saves up a lot of time/complexity.

