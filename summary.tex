%
% LATEXBONES
%
\documentclass[a4paper,11pt,twoside]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage[applemac]{inputenc}
\usepackage[colorlinks,bookmarks=false,linkcolor=blue,urlcolor=blue]{hyperref}
\usepackage{subfigure}
\usepackage{here}
\usepackage{wrapfig}
\usepackage{fancyhdr}
\usepackage{dirtytalk}

%drow graph
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{capt-of}

% print code
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{verbatim}

% push at the bottom
\newenvironment{bottompar}{\par\vspace*{\fill}}{\clearpage}

% landscape
\usepackage{pdflscape}

\paperheight=297mm
\paperwidth=210mm

\setlength{\textheight}{235mm}
\setlength{\topmargin}{-1.2cm} 

\setlength{\parindent}{0pt}

\setlength{\textwidth}{15cm}
\setlength{\oddsidemargin}{0.56cm}
\setlength{\evensidemargin}{0.56cm}

% quotes
\usepackage{framed}
\newcommand*{\signed}[1]{%
  \unskip\hspace*{1em plus 1fill}%
  \nolinebreak[3]\hspace*{\fill}\mbox{#1}
}

\pagestyle{plain}

% --- equations ---
\def \be {\begin{equation}}
\def \ee {\end{equation}}
%\def \dd  {{\rm d}}m

% --- links ---
\newcommand{\mail}[1]{{\href{mailto:#1}{#1}}}
\newcommand{\ftplink}[1]{{\href{ftp://#1}{#1}}}






% ======= Document ======

%----------------------------------------------------------------------------------------
% HEADING SECTIONS
%----------------------------------------------------------------------------------------

% --- header ---
\fancyhead[L]{Applied Data Analysis}
\fancyhead[R]{Summary}

\begin{document}
\begin{titlepage} %Titre
\begin{center}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
\center % Center everything on the page
 
 
 %----------------------------------------------------------------------------------------
% TITLE SECTION
%----------------------------------------------------------------------------------------




\begin{figure} [h] %----------- SubGraph ---------------------
\centerline{
\subfigure{\includegraphics[height = 2 cm]{./pic/EPFL.png}  }
\subfigure{\includegraphics[height = 2 cm]{./pic/ADA-logo.png}} 
} 
\end{figure}


\vspace{0.5cm}
%\textsc{\LARGE EPFL}\\[1.5cm] % Name of your university/college
\textsc{\Large School Of Computer And Communication Sciences}\\[0.5cm] % Major heading such as course name
\textsc{\Large }\\% Minor heading such as course title
%\textsc{ \Large Master Semester Project}\\ % Minor heading such as course title


\HRule \\[0.4cm]
{ \huge \bfseries Applied Data Analysis \\Summary}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

% ---- Lovelace -----
\begin{center}
\includegraphics[width = 5 cm]{pic/lovelace} % Include a department/university logo - this will require the graphicx package
\end{center}



\begin{bottompar}
% ---- Professor -----
\begin{flushleft} \large
Prof. \textsc{Catasta} Michele\\
Distributed Information Systems Laboratory (LSIR) \\
\mail{michele.catasta@epfl.ch} \\ 
\end{flushleft}

% ---- date
{\large June 10, 2016}\\[1cm] % Date, change the \today to a set date if you want to be precise

\end{bottompar}
 
%%----------------------------------------------------------------------------------------
%% AUTHOR SECTION
%%----------------------------------------------------------------------------------------

%
%
%
%
%
%\begin{minipage}[t]{0.4\textwidth}
%
%\end{minipage}
%~
%\begin{minipage}[t]{0.55\textwidth}
%%\begin{flushright} \large
%%\emph{Assistant:} \\
%%\textsc{Voirol} Nicolas \\
%%PhD student\\
%%\mail{nicolas.voirol@epfl.ch} \\ [0.4cm]
%%
%%\emph{Supervisor:} \\
%%\textsc{Kuncak} Viktor\\  % Supervisor's Name
%%Professor\\
%%LARA - Laboratory for Automated Reasoning and Analysis\\
%%\mail{viktor.kuncak@epfl.ch}
%%\end{flushright}
%\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
% LOGO SECTION
%----------------------------------------------------------------------------------------



%----------------------------------------------------------------------------------------
% DATE SECTION
%----------------------------------------------------------------------------------------

%{\Large \today}\\[1cm] % Date, change the \today to a set date if you want to be precise


%
%\begin{center}
%\abstract{\large Experiment various methods to compare functional trees between them. Given a function, use these algorithms to find the most similar tree contained in a corpus of functions. Try to suggest an autocompletion for a "hole" in a tree, based on this corpus.}
%\end{center} 
 %{\Large IDQ CONFIDENTIAL}
%----------------------------------------------------------------------------------------
\vfill % Fill the rest of the page with whitespace

\end{center}
\end{titlepage}



\pagestyle{fancy}
% ================ Table of content ==============
\newpage
\tableofcontents 

\baselineskip=16pt
%\parindent=15pt
%\parskip=5pt

\newpage




% ================ Introduction ==============
\section{Introduction}

\subsection{General information about the course}

This course covers multiple topics in the data science field such as \textbf{Data Wrangling}, {\bf Data Management}, {\bf Data Mining}, {\bf Machine Learning}, {\bf Visualization}, {\bf Statistics} and {\bf Story telling}. It's about {\bf breadth}, not depth. Indeed, Data science is evolving really quickly, hence learning in depth a specific tool won't pay off. 

\subsection{Data Science}

When we talk about Data Science, we often use the term Big Data as the enormous amount of data that exist in the world. But Big Data is not only about collecting huge amount of data. It is challenging but not enough. The real value comes from the insights. The {\it internet} companies (Google, Facebook, etc.) 
understood this many years ago.
\\ \\
An accurate definition of Data Analysis is given by Wikipedia:
\begin{framed}
{\it {\bf Analysis of data} is a process of {\bf inspecting}, {\bf cleaning}, {\bf transforming}, and {\bf modeling data} with the goal of {\bf discovering useful information}, suggesting conclusions, and supporting decision-making.
Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, {\bf in different business}, science, and social science {\bf domains}.}
\signed{\href{https://en.wikipedia.org/wiki/Data\_analysis}{Wikipedia - Data Analysis}}
\end{framed}

Therefore, a Data Scientist has to master different kind of skills such as {\bf Mathematics} (for the Statistics), {\bf Programming} and the {\bf Domain Expertise}. Drew Conway's Venn diagram, Figure \ref{img:venn}, shows the different combination man can obtain with these three skills.

\begin{figure}[H]
 \centering
 \includegraphics[width=7cm]{./pic/Data_Science_VD.png}
 \caption{\label{img:venn} Venn Diagram describing the different combination of skills used by a Data Scientist (by Drew Conway)}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[width=10cm]{./pic/tweet_wills.png}
 \caption{\label{img:tweet_wills} A tweet from Josh Wills, Data Scientist at Slack.}
\end{figure}

{\bf A practical definition of Data Science} 

Data Science is about the whole processing pipeline to extract information out of data. As such, a Data Scientist {\bf understands and cares about the whole data pipeline}.

\begin{minipage}{0.5\textwidth}
\begin{figure}[H]
 \centering
 \includegraphics[width=8cm]{./pic/pipeline.png}
\end{figure}
\end{minipage} \hfill
\begin{minipage}{0.45\textwidth}
A {\bf data pipeline} consists of 3 steps:
\begin{enumerate}
 \item Preparing to run a model. \\
  {\it Gathering, cleaning, integrating, restructuring, transforming, loading, filtering, deleting, combining, merging, verifying, extracting, shaping}
 \item Running the model
 \item Communicating the results
\end{enumerate}
\vspace{0.5cm}
 A ``good'' Data Scientist will always go back and forth between the steps. The diagram on the left shows exactly what can happen. 
\end{minipage}
\\ \\
In this course, you will develop the following skills:
\begin{description}
 \item[data muning/scraping/sampling/cleaning] in order to get an informative, manageable data setlength
 \item[data storage and management] in order to be able to access data quickly and reliably during subsequent analysis
 \item[exploratory data analysis] to generate hypotheses and intuition about the data
 \item[prediction] based on statistical tools such as regression, classification, and clustering
 \item[communication of results] through visualization, stories and interpretable summaries
\end{description}


% ================ Definition ==============
% maybe to be moved at the end if it becomes a dictionnary
\section{Basic concepts}

A data science student is attended to understand the \textbf{Grammar of Data Science}. Having some backgrounds in SQL concepts is also a good thing because, as it is very common, people loves to make example with it. Here is a brief refresh of some definitions and concepts about data science.

\begin{itemize}
 \item {\bf Structured data} requires two key concepts:
 \begin{itemize}
  \item {\bf Data model} is a collection of concepts for describing data.
  \item {\bf Schema} is a description of a particular collection of data, using a given data model.
 \end{itemize}
 \item The {\bf Relational model} is on of the most common approach to manage data (SQL like) and can handle most of the data. A counter example is the facebook-like data which requires {\bf graph model}. This model is made of 2 parts:
 \begin{itemize}
  \item The {\bf Schema}. \\ For example, \verb+Students(sid: string, name:string, age:integer)+
  \item The {\bf Instance}, {\it i.e.} the data at a given time. \\
  Definitons:
  \begin{itemize}
   \item {\bf Cardinality} is the number of rows. (Number of items)
   \item {\bf Degree} or {\bf Arity} is the number of fields. (Number of attributes)
  \end{itemize}
 \end{itemize}
 \item Definitions of some ``Database'' terms:
 \begin{itemize}
  \item A {\bf JOIN} is a mean to combine tables based on shared attributes (most of the time some \textbf{IDs}). Despite its apparent simplicity beware of the many ways to compute a JOIN and check what is the default JOIN of a language before using it. The FIG \ref{join_SQL} summarises these possibilities.
  \item \textbf{Aggregation}, \textbf{reduction}, and {\bf groupby} are the action of reducing data with a common opperation (\textbf{sum}, \textbf{count}, \textbf{average}, ...) to summarise them. 
 \end{itemize}
 \item Definitions of some ``Pandas'' terms:
 \begin{itemize}
  \item {\bf Series} are a name, ordered dictionary
  \begin{itemize}
   \item keys are indexes
   \item built on \textbf{numpy.ndarray} (so values can be any Numpy data type)
  \end{itemize}  
  \item \textbf{DataFrame} is a table with named column
  \begin{itemize}
   \item the columns are series
   \item it is indeed a dictionary with (columnName $\rightarrow$ series)
  \end{itemize}  
 \end{itemize}
\end{itemize}

\begin{figure}%---------------FIG--------------
 \centering
 \includegraphics[width=12cm]{./pic/SQL_joins}
 \caption{\label{join_SQL} Different ways to join two tables and the related SQL command.}
\end{figure}


\subsection{Panda vs SQL}

Panda is built to allow easy and fast \textbf{data exploration} and not to be a database manager, as SQL is. Thus there are benefits and drawbacks of using it.


\begin{center} %---------------TAB--------------
\begin{tabular} {| l | l |}
\hline
\bf Pros & \bf Cons \\ \hline
Lightweight \& fast & Tables stored directly in memory \\
Great expressivness (combine SQL + Python) & No post-load indexing functionality\\
Easy plot for data visualization (eg Matplotlib) & No transactions, journalings\\ 
& Large, complex joins are slower \\ \hline
\end{tabular}
\end{center}

\subsection{OnLine Analytical Processing (OLAP cubes)}

OLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. Conceptually, it is like an n-dimensional spreadsheet (a cube) on which we can apply various opperations to take decisions.

OLAP cubes are an other way to see data table and are contructed based on them, as shows FIG \ref{OLAP_cubes}.

\begin{figure}[H]%---------------FIG--------------
 \centering
 \includegraphics[width=12cm]{./pic/OLAP_cube}
 \caption{\label{OLAP_cubes} Construction of an OLAP cube from a table.}
\end{figure}

Operations on OLAP cubes are the following and are illustrated on FIG \ref{OLAP_operations}
\begin{itemize}
	\item \textbf{Slicing} fixes one or more variable
	\item \textbf{Dicing} selects a range of one or more variable
	\item \textbf{Driling up/down} changes levels of a hierarchically-indexed variable, ie "zoom" on a variable and see the sub-categories it contains.
	\item \textbf{Pivoting} change the point of view of the cube. Swap an aggregated variable an a detailed one.
\end{itemize}

\begin{figure} [h] %----------- SubGraph ---------------------
\centerline{
\subfigure[Slincing\label{olap_slicing}] {\includegraphics[width=7cm]{pic/Slicing} }
\subfigure[Dicing\label{olap_dicing}] {\includegraphics[width=7cm]{pic/Dicing} } 
}
\centerline{
\subfigure[Driling up/down\label{olap_driling}] {\includegraphics[width=7cm]{pic/Drilling_up_down} }
\subfigure[Pivoting\label{olap_pivoting}] {\includegraphics[width=7cm]{pic/Pivoting} } 
}
\caption{\label{OLAP_operations} Operations on OLAP cubes} 
\end{figure}

\begin{center} %---------------TAB--------------
\begin{tabular} {| l | l |}
\hline
\bf Pros & \bf Cons \\ \hline
& \\
\parbox[t][][t]{7cm}{The main adventage of OLAP cubes is that their are \textbf{conceptualy simpler} to understand by a non-scientist person, eg a business man who have to take day-to-day decisions based on company's data. Aggregations are limited but cover the main common cases that we can encounter.
}&
\parbox[t][][t]{7cm}{Because of the "on-line" behaviour of this approach, all type of aggregation must be pre-calculated amoung all combination of axis which is very \textbf{expensive in memory and in time} (when updating the data)}\\
& \\
\hline
\end{tabular}
\end{center}

% ================ Data Wrangling ==============
\section{Data Wrangling}

\begin{figure}[H]%---------------FIG--------------
 \centering
 \includegraphics[width=12cm]{./pic/path_data_wrangling}
 \caption{\label{path_data_wrangling}Things do not always happen as expected...}
\end{figure}

Before any analysis, data need to be transformed from "dirty" to clean and processable data. 

Data comes from different sources (excel or SQL?), sometime collected through different methods over time, with different conventions (space or NaN?), etc ... Data wrangling's goal is to \textbf{extract and standardize these raw data}. The best way to do it is to \textbf{combine automation with visualizations} in order to find outliers. 

Data's problem can come from (non-exhaustive): 
\begin{itemize}
  \item Missing data
  \item Incorrect data
  \item Inconsistent representations of the same data
  \item Non-standardized data (centimeter or inches? farenheit or celsuis ?)
  \item Duplicated data
\end{itemize}

About 75\% of theses problem will need \textbf{human intervention} to be corrected (by the data-scientist or by crowdsourcing).

Even if it seems really dirty, \textbf{beware not to over-sanitize the data!}. Applying what we can call "defensive programming" is not a good idea because we risk to lose any interesting data, keeping only the ones that fit perfectly in our model.

\subsection{Diagnosis of the data}

One of the most important aspect of Data Wrangling is to {\bf understand} the data and to {\bf find possible problems}. In order to ``diagnose'' the data, two tools can be used:
\begin{itemize}
 \item {\bf Visualization} (A {\it toughtful} visualization will always help)
 \item {\bf Basic Statistics} 
\end{itemize}

Matrix visualizations of the facebook graph is shown in Figure \ref{pic:fb}. The Relational visualization, Figure \ref{pic:fb1}, does not show any particular problem in the data. But the Time dependant visualization, Figure \ref{pic:fb2}, shows that the Facebook API reached its limit while collecting data.

\begin{figure} [h] %----------- SubGraph ---------------------
\centerline{
\subfigure[Relational visualization \label{pic:fb1}] {\includegraphics[width=7cm]{pic/fb1} }
\subfigure[Time dependant visualization\label{pic:fb2}] {\includegraphics[width=7cm]{pic/fb2} } 
}
\caption{\label{pic:fb} Matrix visualization of the facebook graph.} 
\end{figure}


\subsection{Dealing with missing values}

Values can often miss from the data we have, because of various events (war, fire, ...). We must detect and correct these values with different method according with the domain we are working in.

Whatever the method used, it's good to keep track of these changes to know which are original data and which are modified ones.

\begin{itemize}
  \item Set values to zero FIG \ref{miss_val}(a)
  \item Interpolate based on existing data FIG \ref{miss_val}(b)
  \item Omit missing data FIG \ref{miss_val}(c)
  \item Interpolation with track kept \ref{miss_val}(d)
\end{itemize}

\begin{figure}[H]%---------------FIG--------------
 \centering
 \includegraphics[width=15cm]{./pic/missing_values}
 \caption{\label{miss_val}To deal with missing values.}
\end{figure}

\subsection{General procedure}

Once the data are well wrangled and before trying to analyse them we must take care of two more steps:

\begin{enumerate}
  \item \textbf{Deal with uncertain data} (can arise from measurement errors, wrong sampling strategies, etc.)
  \item \textbf{Parse/trasform data} (with aggregation and reduction techniques) to obtain meaningful records
\end{enumerate}
 
It's always ideal to have the code and/or the documentation about the dataset you are analyzing (provenance).
 
% ================ Data Variety ==============
\section{Data Variety}

The ``3 Vs'' of Big Data: {\it Volume}, {\it Velocity} and {\bf Variety}. In this course, we don't address the {\it Volume} and {\it Velocity} parts (A course on Database does). Since there is a lot of variety in the data, we need to prepare the data. This flow is called {\bf ETL}:
\begin{itemize}
 \item {\bf Extract} from the {\it source(s)}.
 \item {\bf Transform} data at the source, sink, or in a {\it staging area}.
 \item {\bf Load} data into the {\it sink}.  
\end{itemize}
The sources can be really different, {\it e.g.} files, database, logs, etc. The Data ranges comes from very stru data in databases, for example, to totally unstructered data such as web pages.

\subsection{Role of Schema}
The {\bf Schema}, which specifies the {\it structure} and {\it types} of data repository, is changing. Traditional databases are {\bf schema-on-write}, {\it i.e.} you cannot load data into a table without a schema. But new data stores (NoSQL for example) are {\bf schema-on-read} or {\bf schemaless}. You can either have a schema only when you read the data or just avoid having a schema. Youtube and Google Cache are some examples of schemaless data storage.
\\\\
SQL is a {\bf schema-on-write} data type, {\it i.e.} we must create a table before inserting anything. XML is a {\bf schema-on-read} data type. If the data are stored without a schema, everything is simply stored as a string. We need to use a parser to return a typed data. 

\subsection{Examples of data}

\subsubsection{XML and DOM}

The XML data are used mostly with HTML and specifies the data structure. An XML schema can be applied to interpret the XML data and specifies the {\bf data types}. Figure \ref{pic:xml} shows the XML data \ref{pic:xml_data} and the schema \ref{pic:xml_schema} used to parse and type the data.

\begin{figure} [h] %----------- SubGraph ---------------------
\centerline{
\subfigure[XML data \label{pic:xml_data}] {\includegraphics[width=5cm]{pic/xml_data} }
\subfigure[XML schema\label{pic:xml_schema}] {\includegraphics[width=7cm]{pic/xml_schema} } 
}
\caption{\label{pic:xml} Example of XML.} 
\end{figure}

The XML is a text format that encodes {\bf DOM} (Document-Object Models). It's a data structure often used by Web pages. The DOM is tree-structured. An example of a DOM is given in Figure \ref{pic:dom}.

\begin{figure}[H]%---------------FIG--------------
 \centering
 \includegraphics[width=10cm]{./pic/dom}
 \caption{\label{pic:dom} Example of a DOM tree for an HTML Web page.}
\end{figure}

THe XML schema allows a database to interpret the data when running queries. It can do arithmetic or range queries on numerical values, for example.

\subsubsection{JSON}

{\bf JSON} stands for Javascript Object Notation. It's a schemaless data. Schema support was added later. An example of JSON data is shown in Figure \ref{pic:json}

\begin{figure}[H]%---------------FIG--------------
 \centering
 \includegraphics[width=8cm]{./pic/json}
 \caption{\label{pic:json} Example of a JSON data.}
\end{figure}

JSON is typically used to represent {\bf hierachical data structures} directly in the target language (Javascript or Java at the beginning). The transformation on the data are procedural in the target languages. It is often easier for some tasks. But it can be painful for the schema changes, for example.

\subsubsection{Tabular data}

A Tabular Data is simply data put into a table such as CSV or TSV. Definition of a table:
\begin{itemize}
 \item A {\bf table} is a collection of {\bf rows} and {\bf columns}.
 \item Each row has an {\bf index}.
 \item Each column has a {\bf name}.
 \item A {\bf cell} is specified by an (index, name) pair.
 \item A cell may or may not have a {\bf value}.
\end{itemize}

It's a very simple yet powerful data type. For example, the sensors usually output data in the form of time series, transformed into a tabular format. However, a system dealing with sensor data should:
\begin{itemize}
 \item support both long-term ({\bf trend}) and short-term ({\bf real-time}) queries
 \item have {\bf low latency} but also efficient. It should use {\bf real-time indexing} for longer-term queries.
 \item support triggers ({\bf alerts}) for a variety of conditions.
\end{itemize}
Therefore, the {\bf complexity of a data format} does not determine the {\bf complexity of the system required to properly handle it}.

\subsubsection{Log files}

The log files are simple text files giving information about process. The daemons, such as \verb+httpd+, \verb+mysqld+ or \verb+syslogd+, usually create logs. {\bf \verb+syslog+} was developed by Eric Allman. It's a way for devices to send event messages to a server that will log all the events. Splunk is a company that built a successfull business model around the syslog events.

\subsubsection{Binary formats}

They are often the key to performance because we {\bf avoid expensive parsing}. The modern formats even support nested structures, various level of schema enforcement, {\bf compression}, etc. Somes examples: Protocol Buffers (Google), Avro, Parquet, etc.

\subsection{Processing the data (JSON and XML)}

In order to process XML, we can use the DOM. It can also be used to process JSON data. The DOM is very easy to work with: all the data are directly accessible by links. The problem is that we {\bf might not care about most of the data} and if the data are big, they {\bf might not fit into the RAM}. In order to deal with these two problems, we can use a {\bf SAX} parser which is an event-driven parser. It will find all the {\bf open-close-tag events} in an XML document and will {bf do callbacks to user code}.

\begin{itemize}
\renewcommand{\labelitemi}{{\bf +}}
 \item User code can respond to only a subset of events corresponding to the tag it is interested in.
 \item User code can correctly compute aggregates from the data rather than create a record for each tag.
 \item User code can implement flexible error recovery strategies for ill-formed XML.
\renewcommand{\labelitemi}{{\bf --}}
 \item User code must implement a state machine to keep track of ``where it is'' in the DOM tree.
\end{itemize}

For JSON, most parsers construct the ``DOM'' directly. But there are a few SAX-style parsers: Jackson, JSON-simple, etc. Sometimes {\bf SAX-style is the way to handle ill-formed datasets}, an endless array of objects for example.

\subsection{HTML and Web Services}

\subsubsection{HTML}

Internet contains an ``enormous'' amount of data. Some crawlers such as Common Crawl dataset contains about 1.82 billions web pages (for 145 TB). We can use different tools to crawl data from the web. Examples for Python: Beautiful Soup, Requests, Scrapy, etc. 
\\\\
Most of the time, the Web pages are considered as unstructered data. But you can find some semi-structured data, {\it e.g.} Google WebTables. Some big ``internet'' companies (Google, Yahoo, Yandex and Microsoft) are sponsoring a project called {\bf schema.org} to create structured or semi-structured Web pages. A core vocabulary for the type of fields is given. schema.org is more and more used. It's also used by knowledge bases such as Google Knowledge Graph. {\bf WikiData} is a community project to create an open database of structured data taken from Wikipedia.

\subsubsection{Web Services}

Screen-scraping the content of a large website is really difficult due to the hidden links, forms, etc. Therefore they are providing Web Service APIs\footnote{Application Program Interface: Set of subroutine definitions, protocols, and tools for building software and applications. In this particular case, the APIs are used to retrieve the data from the Web page, {\it e.g.} Facebook API to retrieve the contacts.}. There are two kinds of Web Services:
\begin{itemize}
 \item The old way: XML-based RPC-style messages: SOAP
 \item The new way: REST-style stateless interactions, URLs encode state
\end{itemize}
{\bf RPC}
\\
The SOAP RPC\footnote{SOAP = Simple Object Access Protocal, RPC = Remote Procedure Call} messages typically encode arguments that are presented to the calling profram as parameters and return values. HTTP POST/GET are used to communicate.
\begin{figure}[H]%---------------FIG--------------
 \centering
 \includegraphics[width=13cm]{./pic/soap-rpc}
 \caption{\label{pic:soap-rpc} Example of a SOAP RPC exchange.}
\end{figure}
This kind of procedure (same for XML-RPC) requires a request-response cycle. This often leads to longer ``conversations''. The RPC-style is being quickly superseded by newer and more user-friendly technologies.\\\\
In {\bf RPC systems}, the design emphasis is on {\bf verbs}. It uses functions such as {\it getUser()}, {\it addUser()}, etc.

%---------------NEWPAGE--------------
\newpage
{\bf REST}
\\
REST\footnote{REpresentation State Transfer} is a {\bf stateless} client/server protocol. The principles are:
\begin{enumerate}
 \item Each message in the protocol contains all the information needed by the receiver to understand and/or process it. This constraint attempts to {\it ``keep things simple''} and avoids needless complexity.
 \item Set of Uniquely Addressable Resources
 \begin{itemize}
  \item {\it ``Everything is a Resource''} in a RESTful system
  \item Requires universal syntax for resource identification, {\it e.g.} URI.
 \end{itemize}
 \item Set of Well-Defined Operations that can be applied to all resources
 \begin{itemize}
  \item In the context of HTTP (REST APIs), the primary methods are: \\
  {\bf POST}, {\bf GET}, {\bf PUT}, and {\bf DELETE} \\
  These are similar (but not exactly) to the database notion of CRUD (Create, Read, Update, and Delete)
 \end{itemize}
 \item The use of Hypermedia both for Application Information and State Transitions
 \begin{itemize}
  \item Resources are typically stored in a structured data format that supports hypermedia links, such as XHTML or JSON.
 \end{itemize}
\end{enumerate}
In {\bf REST systems}, the design emphasis is on {\bf nouns}. It uses the HTTP Protocols (POST, GET, PUT, and DELETE) a {\it User}, a {\it Location}, etc. 



%======= TABLEAU ===========
%\begin{center} %---------------Tab--------------
%\begin{tabular} {| c | c | c | c | c | c |}
%\hline
 %& & & & & $\\ \hline
%\end{tabular}
%\end{center}


%===========GRAPH================
%\begin{figure} %---------------------Graph---------------------------
%\begin{center}
%\includegraphics[width=12cm]{graph/ampli2} 
%\end{center}
%\caption{\em  \label{label}
%L�gende
%}
%\end{figure}


%========SUBGRAPH=======
%\begin{figure} [h] %----------- SubGraph ---------------------
%\centerline{
%\subfigure[ sublegend ] {\label{sfig:thetat} \includegraphics[width=7cm]{ graph/graph_convdt3 } }
%\subfigure[ sublegend ] {\label{sfig:thetafin} \includegraphics[width=7cm]{ graph/graph_convtfin } } 
%}
%\caption{\label{ label } 
%L�gende
%} 
%\end{figure}








\end{document} %%%% THE END %%%%
